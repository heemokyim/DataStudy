{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate() # default spark context 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() \n",
    "# sparkContext를 여러개 만들면 안돌아간다. 그래서 새로운 context생성시 먼저 stop을 해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'local[2]', appName = 'Spark Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd  = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd 는 가상의 리스트다. 무엇인가를 가지고있다는 정보만 들고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x >1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter.map.map 이렇게 계속하다가 실제 연산을 원하면 count, action 을 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map,filter 많이쓰이는 transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # rdd 를 뽑아보면, 123,으로만들었으니까 1,2,3 그대로있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x:x*2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 의 list 를 하나의 list 로 줄이는것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llist = [[1,2],[3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 =sc.parallelize(llist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 2], [3, 4], [3, 4]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이것을 하나의 리스트로 \n",
    "rdd2.flatMap(lambda x:(x,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[27] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.intersection(rdd2)# 트랜스포메이션이기때문에 계산도안하고, 시간도안걸림 따라서 collect를 아래서 달아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect 는 데이터가 아주 작을때, 사용해야함, collect 는 rdd 의 결과를 리스트로 반환.\n",
    "#take(n) 도 많이쓰임 데이터 조회용.\n",
    "# reduce 는 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce 는 데이터를 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([\"1\",\"2\",\"3\",\"4\",\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12345'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하지만 리듀스는,  순서를 보장하지않음.\n",
    "# 매핑과 리듀스 두가지만으로도 데이터 분석을 많이 할 수 있다.\n",
    "#reduce 는 액션이다, reducebykey는 데이터가 키랑 벨류로 되어있을때 reduce 하는 방법이다. 리듀스 바이 키는 트랜스포메이션이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rdd = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2132d28dba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setMaster(\"local[2]\").setAppName(\"Conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5]) # rdd 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # 기본적인 rdd를 생성한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4, 5]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect() # partition단위로 작업을 한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd.glom().collect()) \n",
    "# partition을 2개로 만들어버림. 나중에 파일 2개로 반환해줄것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5], 3) # partition을 3개로 나눠서 rdd 객체 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sc.parallelize(\"./sample/text-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', '/', 's', 'a', 'm', 'p', 'l', 'e', '/', 't', 'e'],\n",
       " ['x', 't', '-', 't', 'e', 's', 't', '.', 't', 'x', 't']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.glom().collect() # partition을 나눠서 한 글자씩 받아옴.\n",
    "#glom은 각 partition마다 element들을 나눠서 리턴해준다. = transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '/',\n",
       " 's',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " '/',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " '-',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " '.',\n",
       " 't',\n",
       " 'x',\n",
       " 't']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect() # 한 글자씩 다 받아옴.\n",
    "#collect는 모든 element들을 다 받아오는 것이다. = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"], 3) \n",
    "# partition 3개로 나눠서 해당 리스트 rdd 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddMap = rdd.map(lambda x:(x,1)) # mapping해줌.\n",
    "# 특정 형태( key, value 쌍)로 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 1)], [('b', 1)], [('c', 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rddMap.glom().collect()) \n",
    "# partition 3개로 나뉘어져 있는 mapping결과를 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.filter(lambda x:x%2==0).collect() # rdd객체에서 filtering한 객체를 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatMap의 경우 각 RDD에 적용시 partition단위로 출력하며, 없는 데이터는 삭제해준다.\n",
    "\n",
    "rdd = sc.parallelize([2,3,4])\n",
    "rdd = rdd.map(lambda x:range(1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[range(1, 2)], [range(1, 3), range(1, 4)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([2,3,4])\n",
    "rdd.flatMap(lambda x:range(1,x)).collect()\n",
    "\n",
    "# flatMap의 경우 Mapping해준뒤 1열로 하나의 리스트에 한개씩 쭉 나열해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"Roses are red\", \"Violets are blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Roses', 'are', 'red'], ['Violets', 'are', 'blue']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x:x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Roses', 'are', 'red', 'Violets', 'are', 'blue']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"Roses are red\", \"Violets are blue\"])\n",
    "rdd.flatMap(lambda x:x.split()).collect()\n",
    "# flatMap의 경우 Mapping해준뒤 1열로 하나의 리스트에 한개씩 쭉 나열해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,1,2,3]).distinct().collect()\n",
    "#distinct()는 중복된 결과를 없애준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample은 일부 샘플만 추출해서 가져온다. RDD의 subset을 추출해줌.\n",
    "\n",
    "rdd = sc.parallelize(range(100), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = rdd.sample(False, 0.1, 12)\n",
    "# 중복 False, 10% 비율, seed =12로 주고 랜덤으로 샘플링 rdd 객체 생성\n",
    "# 다만 비율이 정확한 값이 아니라 대략 10%를 갖고 오는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[19] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 10, 15, 16, 19], [26, 32], [62, 63, 65, 72], [98]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.count() # rdd 객체 내부의 element 개수\n",
    "# count()의 경우 spark내부 함수. 그래서 len()보다는 더 최적화되어 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN\n",
    "x = sc.parallelize([(\"a\",1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\",2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left Outer Join\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 1))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y.leftOuterJoin(x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right Outer Join\n",
    "sorted(x.rightOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join : 키 값이 같은 것들에 대해 join\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection : 교집합해줌.\n",
    "sorted(x.intersection(y).collect()) # 완전히 교집합 되는 것이 없어서 출력되는것이 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reapartition \n",
    "#: 파티션 개수가 다를 경우 join이나 zip할 때 오류가 발생할 수 있어 \n",
    "# 그것을 방지하기 위해 다시 partition 개수를 수정해준다. \n",
    "\n",
    "rdd.glom().count() # 현재 4개의 partition을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84],\n",
       " [20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94],\n",
       " [35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99],\n",
       " [45, 46, 47, 48, 49],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(10).glom().collect() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://localhost:4040/ 으로 들어갈 경우 spark에서 작업을 어떻게 수행했는지 볼수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 14, 70, 99, 60, 78, 80, 13, 33, 43]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take : 하나의 partition에서 상위 몇가지의 element 리턴 = transformation\n",
    "# cache: 따로 작업공간을 만들어줌. 차후에 꼭!! cache공간을 없애주어야함.\n",
    "# 그렇지 않는다면 프로세스 종료후에도 남아있는 경우가 있다.\n",
    "#takeSample : 특정샘플 몇개만 갖고 오는 것. = action\n",
    "\n",
    "rdd.takeSample(False, 10, 123) # action 함수이기 때문에 따로 print해줄 필요가 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduce : key, value 쌍으로 된 것들을 partition 단위로 reduce 시행.\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partition 개수에 따라서 생각하는 operation이 아닌 다르게 나올 수 있기 때문에 조심할것.\n",
    "rdd = sc.parallelize([1,2,.5, .1, 5,.2], 3)\n",
    "rdd.reduce(lambda x,y:x/y)\n",
    "\n",
    "# 0.5   ,  5 , 25\n",
    "# 0.01  , 25\n",
    "# 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [0.5, 0.1], [5, 0.2]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"a\",3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduceByKey : 키 값을 기반으로 value값을 더해서 보여줌.\n",
    "from operator import add\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach : RDD 객체 내의 element들을 한개 씩 python형태로 넘겨주는데 쓰임.\n",
    "# foreachPartition : RDD객체의 partition을 한개 씩 python형태로 넘겨주는데 쓰인다.\n",
    "# saveAsTextFile : RDD객체를 텍스트파일로 저장.\n",
    "\n",
    "rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"a\",3)], 2)\n",
    "rdd.saveAsTextFile(\"abc2\")\n",
    "#위에서 partition을 2개로 나눴기 때문에 2개의 파일에 나뉘어져서 rdd 객체가 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('a', 1)\", \"('b', 2)\", \"('a', 3)\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromText = sc.textFile(\"abc2\") #해당 디렉토리 주소를 인자로 주면 해당파일 불러올 수 있다.\n",
    "fromText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [(\"a\", 22), {\"b\": 23}, [\"c\", 4]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1][\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD는 JVM에서 메모리를 잡고 , python에서는 그냥 통신해서 갖고 오는 것임. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame은 데이터 타입 정의된 테이블 형태로 반환(익숙한 형태의 스키마)해주기 때문에,<br> row, column에 따라 갖고 올 수 있도록 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 또한 immutable. 수정할 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 경우 SparkSQL을 쓸 수 있도록 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.16.131.39:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22da723f60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.16.131.39:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 spark 세션을 만들던 sc context를 만들던 어짜피 쓰는 드라이버는 똑같다.<br>\n",
    "껍데기만 다를 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--------------8.6일---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개가 같다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 와 데이터프레임의 차이는 스키마가 있고 없고, 데이터프레임은 raw 데이터를 스키마가 있게 만들어줌. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sample/ages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 였으면 그냥 들어간데로나온다.\n",
    "\n",
    "read.csv, read.format( 포맷 명시 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"sample/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # 컬럼명이랑 데이터타입을 추론함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#굉장희 명확하게 구조화된 데이터를 불러온다. jdbc 커넥터로 붙일수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"sample/people.json\") # rdd를 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample/people.json MapPartitionsRDD[54] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\":\"Michael\"}',\n",
       " '{\"name\":\"Andy\", \"age\":30}',\n",
       " '{\"name\":\"Justin\", \"age\":19}']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() #똑같은파일인데, rdd 로 가져오면 테이블형식이아님.  데이터프레임으로 되면 : 스키마구조를 갖고 , 자동으로 데이터타입을 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd) #rdd 로 부터 dataframe 을 만든다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rdd 로 부터 데이터프레임을 만들고 sql 문쓰기 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://samsung.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Spark Test>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://samsung.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xe7e9dc86a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize((\n",
    "\"\"\"\n",
    "    { \"id\":123,\n",
    "    \"name\":\"Katy\",\n",
    "    \"age\":19,\n",
    "    \"eyeColor\":\"brown\"\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "    \"id\":124,\n",
    "    \"name\":\"Joe\",\n",
    "    \"age\":44,\n",
    "    \"eyeColor\":\"black\"\n",
    "\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "    \"id\":125,\n",
    "    \"name\":\"Romanson\",\n",
    "    \"age\":25,\n",
    "    \"eyeColor\":\"blue\"\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    { \"id\":123,\\n    \"name\":\"Katy\",\\n    \"age\":19,\\n    \"eyeColor\":\"brown\"\\n    }\\n',\n",
       " '\\n    {\\n    \"id\":124,\\n    \"name\":\"Joe\",\\n    \"age\":44,\\n    \"eyeColor\":\"black\"\\n\\n    }\\n',\n",
       " '\\n    {\\n    \"id\":125,\\n    \"name\":\"Romanson\",\\n    \"age\":25,\\n    \"eyeColor\":\"blue\"\\n\\n    }\\n\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() #문자열 그대로 들어감 .처리하려면 regex로 다짤라줘야함 하지만 json 이라는 파일형식을 가지니까. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd) #rdd 에 있는 json 을 그대로가져옴 , 컬럼 , 데이터타입 추론해서 rdd 로 부터 dataframe 을 만든다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, eyeColor: string, id: bigint, name: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # 추론된 결과가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 에서 쓰던 dataframe 과는 달리, 여기의 dataframe 은 row 단위로 관리한다 , pandas 의 dataframe은 컬럼으로 관리함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=123, name='Katy'),\n",
       " Row(age=44, eyeColor='black', id=124, name='Joe'),\n",
       " Row(age=25, eyeColor='blue', id=125, name='Romanson')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 테이블 형태로 가져옴. \n",
    "df.createOrReplaceTempView(\"test\") #view테이블 만들껀데 view table 의 이름을 test 로 지정 dataframe - > temp view 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df에서 collect 같은 기능이 show 다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+\n",
      "|age|eyeColor| id|    name|\n",
      "+---+--------+---+--------+\n",
      "| 19|   brown|123|    Katy|\n",
      "| 44|   black|124|     Joe|\n",
      "| 25|    blue|125|Romanson|\n",
      "+---+--------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # 스키마를 가지기때문에 스키마 형태로 데이터를 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, eyeColor: string, id: bigint, name: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from test\") #sql 문쓰기 테이블명 만들어줬음 이미 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=123, name='Katy'),\n",
       " Row(age=44, eyeColor='black', id=124, name='Joe'),\n",
       " Row(age=25, eyeColor='blue', id=125, name='Romanson')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from test\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 에서 있던 collect는 스키마없이 컬렉트 반면에, 여기서는 collect가  row 형태로 . 스키마 있는 형태로 리턴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#새로 RDD 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\n",
    "    [\n",
    "        (123,\"katie\",19,\"brown\"),\n",
    "        (124,\"joe\",45,\"black\"),\n",
    "        (125,\"romanson\",25,\"blue\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 'katie', 19, 'brown'),\n",
       " (124, 'joe', 45, 'black'),\n",
       " (125, 'romanson', 25, 'blue')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 그냥 읽으면 컬럼명 같은것을 다 정해줌 , 하지만 여기서는 우리가 정의할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *  # 데이터 타입 다 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name, dataType,nullable,metadata 우리가 정해줄거임.\n",
    "\n",
    "scheme = StructType(\n",
    "    [\n",
    "        StructField(\"id\",LongType(),nullable=True),\n",
    "        StructField(\"name\",StringType(),nullable=True),\n",
    "         StructField(\"age\",LongType(),nullable=True),\n",
    "         StructField(\"eyeColor\",StringType(),nullable=True)\n",
    "    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema 와 rdd 를 합쳐서 -> dataframe 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.createDataFrame(rdd,schema=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=123, name='katie', age=19, eyeColor='brown'),\n",
       " Row(id=124, name='joe', age=45, eyeColor='black'),\n",
       " Row(id=125, name='romanson', age=25, eyeColor='blue')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제부터는 sql 작업가능해짐\n",
    "spark.sql(\"select * from test2\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*)from test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 세가지 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|123| 19|\n",
      "|124| 44|\n",
      "|125| 25|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id,age from test\").show() # sql 문 만쓴거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|125| 25|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",'age').filter(\"age=25\").show() # sql,함수형 혼합해서 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터프레임 ,카탈리스트 옵티마이저로 최적의경로로 데이터를 불러올것이기때문에 어떤식으로 구성하든 상관이없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|125| 25|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id,df.age).filter(df.age== 25 ).show() #완전히 함수형으로할경우, filter 함수씀. 대신 python 문법처럼 써야함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightPath = \"sample/departuredelays.csv\"\n",
    "airportPath = \"sample/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = spark.read.csv(flightPath,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='01011245', delay='6', distance='602', origin='ABE', destination='ATL')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = spark.read.csv(airportPath,header=True,inferSchema=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abbotsford', State='BC', Country='Canada', IATA='YXX')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#공항정보 , 비행정보 값을 join 하려함.\n",
    "airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql 쿼리쓰기위해 tempView 를 만들어준다. table 명이 없으니까\n",
    "flight.createOrReplaceTempView(\"flight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport.createOrReplaceTempView(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   city|origin|   delay|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql( # 두 테이블 join 한다. 도시별 delay 를 보려고\n",
    "    \"\"\"\n",
    "        select a.city, f.origin,sum(f.delay) as delay from flight f join airport a on a.IATA = f.origin\n",
    "        where a.State = \"WA\"\n",
    "        group by a.City,f.origin\n",
    "        order by delay desc\n",
    "\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F # sql 의 내부함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport.orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-bf3204d36289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 함수형태로 쓰면\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mairport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflight\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mairport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIATA\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mState\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"WA\"\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum(delay)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column is not iterable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;31m# string methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "# 함수형태로 쓰면\n",
    "airport.join(flight ,airport.IATA==flight.origin)\\\n",
    "        .where(airport.State == \"WA\")\\\n",
    "        .select(airport.City,flight.origin,sum(flight.delay))\\\n",
    "        .groupby(airport.City,flight.origin)\\\n",
    "        .agg(F.sum(flight.delay))\\\n",
    "        .orderBy(\"sum(delay)\",ascending=False)\\\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   City|origin|sum(delay)|\n",
      "+-------+------+----------+\n",
      "|Seattle|   SEA|  159086.0|\n",
      "|Spokane|   GEG|   12404.0|\n",
      "|  Pasco|   PSC|     949.0|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.join(flight, airport.IATA == flight.origin)\\\n",
    ".where(airport.State == \"WA\")\\\n",
    ".select(airport.City, flight.origin, flight.delay)\\\n",
    ".groupby(airport.City, flight.origin)\\\n",
    ".agg(F.sum(flight.delay))\\\n",
    ".orderBy(\"sum(delay)\", ascending=False)\\\n",
    ".show()\n",
    " # 오름차순 False -> 내림차순 = 큰값부터 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우리는 로컬에서 작업을 해주지만 , 실질적으로 카탈리스트가 최적의경로로 자료를 뽑아옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = StreamingContext(sc ,5 ) # 5초마다 스트리밍을 받아온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.streaming.context.StreamingContext at 0x9f78f806a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming #스트리밍 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어디서 부터 스트리밍을 받아올것인지 소켓을 정해준다 . 받아온 텍스트를 라인이라고한다.\n",
    "lines = streaming.socketTextStream(\"localhost\",9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines) # 이게 RDD 형태이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line:line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda w:(w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pairs.reduceByKey(lambda a,b : a+b) #키값에 따라 벨류를 더해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 받는다고 가정하고 눈에 안보이는것을 디자인하고있는과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming 언제 끝날것인가. awaitTermination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nc -lvp 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:05\n",
      "-------------------------------------------\n",
      "('apple', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:17:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:00\n",
      "-------------------------------------------\n",
      "('jeungmin', 1)\n",
      "('is', 2)\n",
      "('nice', 1)\n",
      "('taeuk', 1)\n",
      "('babo', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:10\n",
      "-------------------------------------------\n",
      "('likes', 1)\n",
      "('and', 1)\n",
      "('baseball', 1)\n",
      "('taeuk', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:20\n",
      "-------------------------------------------\n",
      "('like', 1)\n",
      "('jeungmin', 1)\n",
      "('macbook', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:18:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:19:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:20:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:21:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:22:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:23:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:24:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:25:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:26:55\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#netcat 은 서버에다가 request  보내고 ,response 받는 용도. 인터넷상에 cat 을 슬수있음.\n",
    "streaming.start()\n",
    "streaming.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 프레임을 이용해서 구조적으로 이용한다\n",
    "lines = spark.readStream.format(\"socket\")\\\n",
    ".option(\"host\",\"localhost\")\\\n",
    ".option(\"port\",9999)\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(explode(split(lines.value,\" \")).alias(\"word\")) # word라는 한컬럼을 가지고있는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount  =  words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = wordcount.writeStream.outputMode(\"complete\")\\\n",
    ".format(\"console\")\\\n",
    ".start()\n",
    "\n",
    "stream.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING 의 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 144.5, 5.9, 33, \"M\"),\n",
    "        (2, 167.2, 5.4, 45, \"M\"),\n",
    "        (3, 124.1, 5.2, 23, \"F\"),\n",
    "        (4, 144.5, 5.9, 33, \"M\"),\n",
    "        (5, 133.2, 5.7, 54, \"F\"),\n",
    "        (3, 124.1, 5.2, 23, \"F\"),\n",
    "        (5, 129.2, 5.3, 42, \"M\"),    \n",
    "    ],\n",
    "    [\"id\", \"weight\", \"height\", \"age\", \"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"id\").show() #id 같은거는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropDuplicates([c for c in df.columns if c != \"id\"]).distinct().count() \n",
    "# id만 잘못되었지만, 데이터는 다른디 날리기가 좀 그르네ㅔ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([c for c in df.columns if c != \"id\"])\n",
    "# 값이 중복된것을 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"id\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([c for c in df.columns if c == \"id\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Count|Dcount|\n",
      "+-----+------+\n",
      "|    7|     5|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    f.count(\"id\").alias(\"Count\"),\n",
    "    f.countDistinct(\"id\").alias(\"Dcount\")  # 중복 제거\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  5| 129.5|   5.2| 44|     M| 601295421440|\n",
      "|  4| 133.5|   5.9| 33|     M| 670014898176|\n",
      "|  3| 144.5|   5.2| 23|     F| 996432412672|\n",
      "|  5| 129.5|   5.2| 42|     M|1108101562368|\n",
      "|  3| 124.5|   5.7| 23|     F|1503238553600|\n",
      "|  2| 167.5|   5.4| 45|     M|1709396983808|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_id\", f.monotonically_increasing_id()).show() # 자동으로 아이디가 겹치지 않게 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"old_id\", df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"id\", f.monotonically_increasing_id()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"old_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning data 에서 계속... 영준씨 필기 참고 \n",
    "\n",
    "\n",
    "### 판다스 안쓰고 이거 쓰는이유는 이것은 실시간 처리가 가능하다는것!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.6, 28, \"M\",100000),\n",
    "        (2, 167.2, 5.4, 45, \"M\",None),\n",
    "        (3, None, 5.2,None,None,None),\n",
    "        (4, 144.5, 5.9, 33, \"M\",None),\n",
    "        (5, 133.2, 5.7, 54, \"F\",None),\n",
    "        (6, 124.1, 5.2, None, \"F\",None),\n",
    "        (7, 129.2, 5.3, 42, \"M\",76000),    \n",
    "    ],\n",
    "    [\"id\", \"weight\", \"height\", \"age\", \"gender\",\"salary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[507] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd # dataframe 안에 있는데 rdd 다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.rdd) # data frame은 안에 rdd 를 가진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원하는 식 뽑아냄\n",
    "\n",
    "df.rdd.map(\n",
    "    lambda row:(row[\"id\"], sum([c == None for c in row]))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row : 로우별 미싱 벨류 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+----+\n",
      "| id|weight|height| age|gender|  _6|\n",
      "+---+------+------+----+------+----+\n",
      "|  3|  null|   5.2|null|  null|null|\n",
      "+---+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"id=3\").show() # 3번을 보자 . 미싱벨류가 많아서 문제다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------+--------------------------------+-----------------------------+--------------------------------+--------------------------------+\n",
      "|(1 - (count(id) / count(1)))|(1 - (count(weight) / count(1)))|(1 - (count(height) / count(1)))|(1 - (count(age) / count(1)))|(1 - (count(gender) / count(1)))|(1 - (count(salary) / count(1)))|\n",
      "+----------------------------+--------------------------------+--------------------------------+-----------------------------+--------------------------------+--------------------------------+\n",
      "|                         0.0|              0.1428571428571429|                             0.0|           0.2857142857142857|              0.1428571428571429|              0.7142857142857143|\n",
      "+----------------------------+--------------------------------+--------------------------------+-----------------------------+--------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    *[1 - f.count(c)/f.count(\"*\") for c in df.columns]\n",
    "      \n",
    "      ).show() # 미싱벨류 갯수. * : 해당컬럼  비율을 통해서 미싱벨류 찾는다 . 1 이아니니까 미싱벨류가 있다. 14프로가 ㅂ미싱밸류."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [] 이렇게 쓰면. 된다내"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "|idRate|        weightRate|heightRate|           ageRate|        genderRate|        salaryRate|\n",
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "|   0.0|0.1428571428571429|       0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    *[(1 - f.count(c)/f.count(\"*\")).alias(c+\"Rate\") for c in df.columns]\n",
    "      \n",
    "      ).show() # 미싱벨류 갯수. * : 해당컬럼  비율을 통해서 미싱벨류 찾는다 . 1 이아니니까 미싱벨류가 있다. 14프로가 미싱밸류. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 높을수록 많이 비어있는것이다.  salary는 미싱벨류가 너무많다 -  > 데이터로서의 가치가없다 71 프로나 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([c for c in df.columns if c!=\"salary\"]) # salary 컬럼을 뺀 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh = 3 ).show() # null value 가 특정값 이상인 애들을 없엔다. null 값이 몇개나 들어가잇는지 보고. thresh = 3 이면 . 널값 3개이상을 날림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모르는 값에 대해서 평균치를 넣을 것이다.\n",
    "means = df.agg(\n",
    "    *[f.mean(c).alias(c) for c in df.columns if c!=\"gender\"]  #gender 은 카테고리라서 평균을 하지않는다. null값 뺴고 평균을 계산한다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+----+\n",
      "| id|            weight|           height| age|\n",
      "+---+------------------+-----------------+----+\n",
      "|4.0|140.28333333333333|5.471428571428572|40.4|\n",
      "+---+------------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 구한 평균값을 빈칸에 넣어주면 평균에 영향을 미치지않는다. 데이터가 크게 영향을 못한다\n",
    "# 아에 삭제를 하지않는것은 다른 컬럼에 중요한 값이 있을 수 도있어서 예를들먼  height  같은거 때문에 없에지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이제 수정하자 , 하지만 rdd 든 dataframe 이든 immutable 이라서 수정할 수없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd  = means.toPandas() # pandas 로 데이터를 받아옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4.0,\n",
       "  'weight': 140.28333333333333,\n",
       "  'height': 5.471428571428572,\n",
       "  'age': 40.4}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd.to_dict('records') # 판다스 쉽게 , 키벨류 로 집어넣는다.  판다스에있는것을 딕셔너리 타입으로 바꾸기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = mpd.to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428572,\n",
       " 'age': 40.4}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd # 키벨류만 남기기 . 리스트라서 0 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd['gender'] = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428572,\n",
       " 'age': 40.4,\n",
       " 'gender': 'X'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "mpd 는 평균데이터 만들어놈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+------+\n",
      "| id|            weight|height|age|gender|\n",
      "+---+------------------+------+---+------+\n",
      "|  1|             143.5|   5.6| 28|     M|\n",
      "|  2|             167.2|   5.4| 45|     M|\n",
      "|  3|140.28333333333333|   5.2| 40|     X|\n",
      "|  4|             144.5|   5.9| 33|     M|\n",
      "|  5|             133.2|   5.7| 54|     F|\n",
      "|  6|             124.1|   5.2| 40|     F|\n",
      "|  7|             129.2|   5.3| 42|     M|\n",
      "+---+------------------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL 에 평균값이 대체됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미싱데이터가 많은 컬럼은 날려버림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 뉴메리컬 데이터가 있을때, \n",
    "# outlier 라고 해서 정규분포로 봤을때 비정상적인   데이터들을 어떻게 할것인가 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계적인 방법에 의해서 날리는것 을 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.3, 28, \"M\"),\n",
    "        (2, 154.2, 5.5, 45, \"M\"),\n",
    "        (3, 324.3, 5.1, 99, \"F\"),\n",
    "        (4, 144.5, 5.5, 33, \"M\"),\n",
    "        (5, 133.2, 5.4, 54, \"F\"),\n",
    "        (6, 124.1, 5.1, 21, \"F\"),\n",
    "        (7, 129.2, 5.3, 42, \"M\"),    \n",
    "    ],\n",
    "      [\"id\", \"weight\", \"height\", \"age\", \"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.approxQuantile(\"weight\",[0.25,0.75],0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[129.2, 154.2]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles # 로어 바운드  , 어퍼바운드 가 생김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = quantiles[1] - quantiles[0] # 이거는 중간값이됨. 그러고나서 바운드를 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [quantiles[0]- 1.5*IQR,quantiles[1]+1.5*IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91.69999999999999, 191.7]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds # weight 에 대해서 통계적으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"weight\", \"height\", \"age\"]\n",
    "bounds = {}\n",
    "\n",
    "for c in col:\n",
    "    quantiels = df.approxQuantile(c, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiels[1] - quantiels[0]\n",
    "    bounds[c] = [quantiels[0]-1.5*IQR, quantiels[1]+1.5*IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼에대해서 제일 높은범위랑 낮은범위랑. 만든다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|(weight < 91.69999999999999)|\n",
      "+----------------------------+\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "+----------------------------+\n",
      "\n",
      "+----------------+\n",
      "|(weight > 191.7)|\n",
      "+----------------+\n",
      "|           false|\n",
      "|           false|\n",
      "|            true|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#원래 데이터에 select를 할건데 , weight 가 우리가만든 바운드에 낮은값 보다 작은지.\n",
    "df.select(df[\"weight\"] < bounds[\"weight\"][0]).show()\n",
    "df.select(df[\"weight\"] > bounds[\"weight\"][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier =  df.select(\n",
    "    *[\"id\"] + [((df[c] < bounds[c][0]) |\n",
    "    (df[c] > bounds[c][1])).alias(c+\"_O\") for c in col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinTable = df.join(outlier, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  7| 129.2|   5.3| 42|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinTable.filter(\"!weight_O\").select(\"id\", \"weight\", \"height\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 느낌표를 써서 !outlier 들을 뺀 값을 가져옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오류가 났지만 다시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 수집 , 스토리지넣어서 크리닝 - > 머신러닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
